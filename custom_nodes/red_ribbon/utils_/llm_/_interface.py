"""
API interface for the LLM integration with the American Law dataset.
Provides access to OpenAI-powered legal research and RAG components.
"""
import logging
from pathlib import Path
import re
from typing import Dict, Any, Callable, List, Optional, Union


from custom_nodes.red_ribbon.utils_ import logger as module_logger, Configs
from ._llm_client import OpenAiClient, OpenAiLLMOutput
from ._embeddings_manager import EmbeddingsManager
from ._load_prompt_from_yaml import load_prompt_from_yaml, Prompt


def _validate_sql(sql_query: str, fix_broken_queries: bool = True) -> Optional[str]:
    """
    Validate and correct a SQL query string.
    
    Args:
        sql_query: The SQL query string to validate
        
    Returns:
        bool: True if valid, False otherwise
    """
    module_logger.debug(f"Validating SQL query: {sql_query}")

    # Check if it's an empty string
    if not sql_query.strip():
        module_logger.warning("Empty SQL query string provided.")
        return None
    
    # Check if it any markdown patterns.
    if "```sql" in sql_query or "```" in sql_query:
        module_logger.warning("SQL query contains markdown code blocks.")
        if fix_broken_queries:
            # Remove markdown code blocks
            sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
            module_logger.info("Removed markdown code blocks from SQL query.")
            module_logger.debug(f"Cleaned SQL query after markdown removal: {sql_query}")
        else:
            module_logger.error("SQL query contains markdown code blocks and fix_broken_queries is False.")
            return None

    # Check if it's got SELECT in it
    if not re.search(r'^\s*SELECT\s', sql_query, re.IGNORECASE):
        module_logger.warning("SQL query does not start with SELECT.")
        return None

    # Check if it's got doubled elements like SELECT SELECT or LIMIT 10 LIMIT 20
    sql_query = re.sub(r'\b(SELECT|LIMIT)\s+\1', r'\1', sql_query, flags=re.IGNORECASE).strip()
    module_logger.debug(f"Cleaned SQL query after doubled elements removal: {sql_query}")
    return sql_query if sql_query else None


class LLMInterface:
    """
    Interface for interacting with LLM capabilities for the American Law dataset.
    Provides a simplified API for accessing embeddings search and RAG functionality.
    """

    def __init__(self, *, resources: dict[str, Any], configs: Configs,) -> None:
        """
        Initialize the LLM interface.

        Args:
            api_key: OpenAI API key
            model: OpenAI model to use
            embedding_model: OpenAI embedding model to use
            data_path: Path to the American Law dataset files
            db_path: Path to the SQLite database
        """
        self.resources = resources
        self.configs = configs

        self.data_path: Path = configs.paths.AMERICAN_LAW_DATA_DIR
        self.db_path: Path = configs.paths.AMERICAN_LAW_DB_PATH
        self.default_system_prompt: str = configs.DEFAULT_SYSTEM_PROMPT


        self.logger: logging.Logger = resources['logger']
        self.openai_client: OpenAiClient = resources['openai_client']
        self.embeddings_manager: EmbeddingsManager = resources['embeddings_manager']

        self.logger.info(f"Initialized LLMInterface successfully.")
        self.logger.debug(f"LLMInterface attributes\n '{dir(self)}'")


    def generate(self, 
                 user_message: str, 
                 custom_system_prompt: Optional[str] = None,
                 temperature: Optional[float] = None, 
                 max_tokens: Optional[int] = None,
                 response_parser: Optional[Callable[[str], Any]] = None
                 ) -> dict[str, Union[str, int]]:
        try:
            llm_output: OpenAiLLMOutput = self.openai_client.get_response(
                user_message=user_message,
                system_prompt=custom_system_prompt or self.default_system_prompt,
                temperature=temperature or self.openai_client.temperature,
                max_tokens=max_tokens or self.openai_client.max_tokens,
                response_parser=response_parser or (lambda x: x)
            )
            response = {
                "response": llm_output.response,
                "total_tokens": llm_output.total_tokens
            }
        except Exception as e:
            self.logger.exception(f"Error in LLM generation: {e}")
            response = {
                "response": f"Error generating response: {str(e)}",
                "total_tokens": 0
            }
        return response


    async def async_generate(self, 
                 user_message: str, 
                 custom_system_prompt: Optional[str] = None,
                 temperature: Optional[float] = None, 
                 max_tokens: Optional[int] = None,
                 response_parser: Optional[Callable[[str], Any]] = None
                 ) -> dict[str, Union[str, int]]:
        try:
            llm_output: OpenAiLLMOutput = await self.openai_client.async_get_response(
                user_message=user_message,
                system_prompt=custom_system_prompt or self.default_system_prompt,
                temperature=temperature or self.openai_client.temperature,
                max_tokens=max_tokens or self.openai_client.max_tokens,
                response_parser=response_parser or (lambda x: x)
            )
            response = {
                "response": llm_output.response,
                "total_tokens": llm_output.total_tokens
            }
        except Exception as e:
            self.logger.exception(f"Error in LLM async generation: {e}")
            response = {
                "response": f"Error generating async response: {str(e)}",
                "total_tokens": 0
            }
        return response



    def ask_question(
        self,
        query: str,
        use_rag: bool = True,
        use_embeddings: bool = True,
        document_limit: int = 5,
        custom_system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Ask a question about American law.
        
        Args:
            query: User's question
            use_rag: Whether to use Retrieval Augmented Generation
            use_embeddings: Whether to use embeddings search for RAG
            document_limit: Maximum number of context documents to include
            custom_system_prompt: Custom system prompt for LLM
            
        Returns:
            Dictionary with the generated response and additional information
        """
        self.logger.info(f"Processing question: {query}")
        default_system_prompt = "You are a legal research assistant specializing in American municipal and county laws."

        if use_rag:
            # Use RAG to generate a response with context
            return self.generate_rag_response(
                query=query,
                use_embeddings=use_embeddings,
                document_limit=document_limit,
                system_prompt=custom_system_prompt
            )
        else:
            # Use OpenAI directly without RAG context
            try:
                llm_output: OpenAiLLMOutput = self.openai_client.get_response(
                    user_message=query,
                    system_prompt=custom_system_prompt or default_system_prompt
                )
                response = {
                    "query": query,
                    "response": llm_output.response,
                    "model_used": self.openai_client.model,
                    "total_tokens": llm_output.total_tokens
                }
            except Exception as e:
                self.logger.exception(f"Error in direct LLM query: {e}")
                response = {
                    "query": query,
                    "response": f"Error generating response: {str(e)}",
                    "model_used": self.openai_client.model,
                    "total_tokens": 0
                }
        return response


    def determine_user_intent(self, message: str) -> str:
        """
        Determine the user's intent based on the query.
        
        Args:
            query: User's input query
            
        Returns:
            Intent type as a string
        """
        # Make sure the response isn't anything nasty.
        response = self.openai_client.client.moderations.create(
            model="omni-moderation-latest",
            input=message,
        )
        if response.results[0].flagged:
            self.logger.warning(f"Message flagged by moderation: {message}")
            return "OTHER"
 
        self.logger.debug(f"Entering Determine Intent")
        system_prompt = """
        You are an intent classifier for a legal search system. Your job is to determine what the user wants to do with their query.
        Classify the user's intent into one of these categories:
        1. SEARCH - User wants to find specific laws or cases
        2. QUESTION - User is asking a legal question that needs explanation
        3. CITATION - User is looking for a specific citation or reference
        4. OTHER - None of the above
        Consider any direct or indirect or general information as a basis for your classification.
        Wrap your final answer in Markdown (e.g. ```plaintext```).
        """
        try:
            response = self.openai_client.client.chat.completions.create(
                model="gpt-3.5-turbo-0125",
                messages=[
                    {"role": "system", "content": system_prompt.strip()},
                    {"role": "user", "content": f"Message: {message}".strip()}
                ],
                max_tokens=1024,
                temperature=0 # Deterministic output
            )
            if response.choices:
                choice_content = response.choices[0].message.content.strip().lower()
                self.logger.debug(f"Determine intent content: '{choice_content}'")

                if "```" in choice_content:
                    choice_content = choice_content.split("```")[1].strip()
                
                self.logger.debug(f"Determine intent content after stripping: '{choice_content}'")

                # Extract intent from the classifier response
                if "search" in choice_content:
                    return "SEARCH"
                elif "question" in choice_content:
                    return "QUESTION"
                elif "citation" in choice_content:
                    return "CITATION"
                elif "other" in choice_content:
                    return "OTHER"
                else:
                    # Default fallback if the response doesn't match any category.
                    # This includes cases where the model returns a malformed or .
                    self.logger.error("Response did not match expected intent categories")
                    self.logger.debug(f"Response content: {choice_content}")
                    return "OTHER"  # Default to search as fallback
            else:
                self.logger.error("No valid choice in the GPT response")
                return "OTHER"  # Default fallback
        except Exception as e:
            self.logger.error(f"Error in determine_intent_basic: {e}", exc_info=True)
            return "OTHER"  # Fallback in case of exception


    def search_embeddings(
        self,
        query: str,
        file_id: Optional[str] = None,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Search for relevant documents using embeddings.
        
        Args:
            query: Search query
            file_id: Optional file ID to limit search to
            top_k: Number of top results to return
            
        Returns:
            List of relevant documents with similarity scores
        """
        # Generate embedding for the query
        query_embedding = self.openai_client.get_embeddings(query)
        
        if file_id:
            # Search in a specific file
            results = self.embeddings_manager.search_embeddings_in_file(
                query_embedding=query_embedding,
                file_id=file_id,
                top_k=top_k
            )
            
            # Add metadata for each result
            enriched_results = []
            for result in results:
                metadata = self.embeddings_manager.get_document_metadata(result['cid'], file_id)
                if metadata:
                    result.update(metadata)
                    enriched_results.append(result)
            
            return enriched_results
        else:
            # Search across all files (limited number)
            return self.embeddings_manager.search_across_files(
                query_embedding=query_embedding,
                max_files=10,  # Limit for performance
                top_k=top_k
            )
    
    def generate_citation_answer(
        self,
        query: str,
        citation_codes: List[str],
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate an answer to a question with specific citation references.
        
        Args:
            query: User's question
            citation_codes: List of citation codes to use as context
            system_prompt: Custom system prompt
            
        Returns:
            Dictionary with the generated response and additional information
        """
        # Collect content from the specified citations
        context_docs = []
        
        for cid in citation_codes:
            # Try to find the document in the database
            doc = self.embeddings_manager.search_db_by_cid(cid)
            
            if doc:
                context_docs.append(doc)
        
        # Build context text
        context_text = "Relevant legal information:\n\n"
        for i, doc in enumerate(context_docs):
            context_text += f"[{i+1}] {doc.get('title', 'Untitled')} - {doc.get('place_name', 'Unknown location')}, {doc.get('state_name', 'Unknown state')}\n"
            context_text += f"Citation: {doc.get('bluebook_citation', 'No citation available')}\n"
            content = doc.get('content', '')
            if content:
                content = content[:1000] + "..." if len(content) > 1000 else content
                context_text += f"Content: {content}\n\n"
        
        # Default system prompt if none provided
        if system_prompt is None:
            system_prompt = """You are a legal research assistant specializing in American municipal and county laws.
Answer the question based on the provided legal citations.
For legal citations, use Bluebook format when available. Be concise but thorough."""
        
        # Generate response using OpenAI
        try:
            response = self.openai_client.client.chat.completions.create(
                model=self.openai_client.model,
                temperature=self.openai_client.temperature,
                max_tokens=self.openai_client.max_tokens,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Question: {query}\n\n{context_text}"}
                ]
            )
            
            return {
                "query": query,
                "response": response.choices[0].message.content,
                "context_used": [doc.get('bluebook_citation', 'No citation') for doc in context_docs],
                "model_used": self.openai_client.model,
                "total_tokens": response.usage.total_tokens
            }
            
        except Exception as e:
            self.logger.error(f"Error generating citation answer: {e}")
            return {
                "query": query,
                "response": f"Error generating response: {str(e)}",
                "context_used": [doc.get('bluebook_citation', 'No citation') for doc in context_docs],
                "model_used": self.openai_client.model,
                "error": str(e)
            }
    
    def query_to_sql(
        self,
        query: str,
        custom_system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Convert a natural language query into a PostgreSQL command for searching the American Law database.
        
        Args:
            query: User's plaintext query
            custom_system_prompt: Optional custom system prompt for the SQL generation
            
        Returns:
            Dictionary with the generated SQL query and additional information
        """
        self.logger.info(f"Converting query to SQL: {query}")
        
        # Define available tables and their schema for context
        schema_info = """
        Available tables and their schema:
        
        1. citations:
           - bluebook_cid (VARCHAR): Unique and primary key CID for citation
           - cid (VARCHAR): CID for the citation's associated law (foreign key)
           - title (TEXT): Plaintext version of the law's title
           - title_num (TEXT): Number in the law's title
           - chapter (TEXT): Chapter title containing the law
           - chapter_num (TEXT): Chapter number
           - place_name (TEXT): Place where the law is in effect
           - state_name (TEXT): State where the place is located
           - state_code (TEXT): Two-letter state abbreviation
           - bluebook_citation (TEXT): Bluebook citation for the law
           
        2. html:
           - cid (VARCHAR): Unique and primary key CID for the law
           - doc_id (TEXT): Unique ID based on law's title
           - doc_order (INTEGER): Relative location of law in corpus
           - html_title (TEXT): Raw HTML of law's title
           - html (TEXT): Raw HTML content of the law
           
        3. embeddings:
           - embedding_cid (VARCHAR): Unique and primary CID for the embedding
           - gnis (VARCHAR): Place's GNIS id
           - cid (VARCHAR): CID for associated law (foreign key)
           - text_chunk_order (INTEGER): Relative location of embedding
           - embedding (DOUBLE[1536]): Embedding vector for the law.
        """
        
        # Default system prompt if none provided
        if custom_system_prompt is None:
            system_prompt = f"""You are a SQL expert specializing in legal database queries.
Your task is to convert natural language questions into PostgreSQL queries.
Use the following database schema information:

{schema_info}

Important guidelines:
1. Always return a valid PostgreSQL query that can be executed directly
2. For full-text search, use the LIKE operator with wildcards (%)
3. Join tables when necessary using the cid field
4. For queries about specific states, filter by state_name or state_code
5. For queries about specific places, filter by place_name
6. Include ORDER BY clauses for relevance
7. When searching text in the html table, use the html field
8. Always include the following fields in the SELECT statement when selecting from the citations table:
   - cid
   - bluebook_cid
   - title
   - chapter
   - place_name
   - state_name
   - date
   - bluebook_citation
9. Use related terms and synonyms based on context clues in the question (e.g. "pets" implies "animals", "dogs", "cats", etc.)

Return ONLY the SQL query without any explanations."""
        else:
            system_prompt = custom_system_prompt
        
        try:
            # Generate SQL using OpenAI
            response = self.openai_client.client.chat.completions.create(
                model=self.openai_client.model,
                temperature=0.2,  # Lower temperature for more deterministic output
                max_tokens=500,   # SQL queries shouldn't be too long
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Convert this question to a PostgreSQL query: {query}"}
                ]
            )
            
            # Extract the SQL query
            sql_query = response.choices[0].message.content.strip()
            
            # Basic validation to ensure it looks like SQL
            if not re.search(r'SELECT|select', sql_query):
                self.logger.warning(f"Generated SQL doesn't contain SELECT statement: {sql_query}")
                sql_query = f"-- Warning: This may not be a valid SQL query\n{sql_query}"

            sql_query = _validate_sql(sql_query)
            if sql_query is None:
                self.logger.error("SQL query validation failed or returned empty. The LLM probably messed up.")
            else:
                self.logger.info(f"SQL query validated successfully")

            output_dict = {
                "original_query": query,
                "sql_query": sql_query if sql_query else "-- Error: No valid SQL query generated",
                "model_used": self.openai_client.model,
                "total_tokens": response.usage.total_tokens,
                "error": None if sql_query else "Invalid SQL query generated"
            }
            return output_dict
        
        except Exception as e:
            self.logger.error(f"Error converting query to SQL: {e}")
            return {
                "original_query": query,
                "sql_query": f"-- Error generating SQL query: {str(e)}",
                "error": str(e)
            }
        
    async def generate_rag_response(
        self, 
        query: str, 
        use_embeddings: bool = True,
        top_k: int = 5,
        system_prompt: Optional[str] = None
    ) -> dict[str, Any]:
        """
        Generate a response using RAG (Retrieval Augmented Generation).
        
        Args:
            query: User query
            use_embeddings: Whether to use embeddings for search
            top_k: Number of context documents to include
            system_prompt: Custom system prompt
            
        Returns:
            Dictionary with the generated response and context used
        """
        # Retrieve relevant context
        context_docs = []
        
        if use_embeddings:
            # Use embedding-based semantic search
            context_docs = await self.search_embeddings(query, top_k=top_k)
        else:
            # Use database text search as fallback
            context_docs = await self.query_database(query, limit=top_k)

        # Build context for the prompt
        context_text = "Relevant legal information:\n\n"
        references = "Citation(s):\n\n"

        for i, doc in enumerate(context_docs):
            context_text += f"[{i+1}] {doc.get('title', 'Untitled')} - {doc.get('place_name', 'Unknown location')}, {doc.get('state_name', 'Unknown state')}\n"
            references += f"{i+1}. {doc.get('bluebook_citation', 'No citation available')}\n"
            context_text += f"Citation: {doc.get('bluebook_citation', 'No citation available')}\n"
            # Limit html to avoid excessively long prompts
            html = doc.get('html', '')
            if html:
                # Turn the HTML into a string.
                content = self.clean_html(html)
                content = content[:1000] + "..." if len(content) > 1000 else content
                context_text += f"Content: {content}\n\n"
        
        # Default system prompt if none provided
        if system_prompt is None:
            system_prompt = """
    You are a legal research assistant specializing in American municipal and county laws. 
    Answer questions based on the provided legal context information. 
    If the provided context doesn't contain enough information to answer the question confidently, 
    acknowledge the limitations of the available information and suggest what additional 
    information might be helpful.
    For legal citations, use Bluebook format when available. Be concise but thorough.
            """

        # Generate response using OpenAI
        prompt: Prompt = load_prompt_from_yaml(
            "generate_rag_response", self.configs, self.logger, query=query, context=context_text
        )
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                messages=[
                    {"role": "system", "content": system_prompt.strip()},
                    {"role": "user", "content": f"Question: {query}\n\n{context_text}"}
                ]
            )
            
            generated_response = response.choices[0].message.content.strip()
            
            # Append the citations
            generated_response += f"\n\n{references.strip()}"
            
            return {
                "query": query,
                "response": generated_response,
                "context_used": [doc.get('bluebook_citation', 'No citation') for doc in context_docs],
                "model_used": self.model,
                "total_tokens": response.usage.total_tokens
            }
            
        except Exception as e:
            self.logger.error(f"Error generating RAG response: {e}")
            return {
                "query": query,
                "response": f"Error generating response: {str(e)}",
                "context_used": [],
                "model_used": self.model,
                "error": str(e)
            }